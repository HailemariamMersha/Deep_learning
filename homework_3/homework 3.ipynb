{"cells":[{"cell_type":"markdown","metadata":{"id":"vKZk65Q7748c"},"source":["# DLSP26U: Introduction to Deep Learning\n","\n","**Name:** [Your Name]\n","\n","\n","**Instructions:**\n","\n","In this notebook, you will be asked to write code and answer questions.  \n","Place all responses under the **Solution** headers.\n","\n","---\n","\n","\n","This third assignment is aimed at coding the multiclass **soft perceptron** algorithm for multiclass classification and the **AdaLinE** algorithm for echo cancellation.\n","\n","Below, we import the necessary libraries and set the random seed for reproducibility."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vc5QAt6p7xxR"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from IPython.display import Audio, display\n","from scipy import signal\n","from scipy.io import wavfile\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","SEED = 0\n","np.random.seed(SEED)"]},{"cell_type":"markdown","metadata":{"id":"9C4mjTLV_apN"},"source":["## 1. Soft Perceptron (multi-class)\n","\n","In the previous assignment, we introduced the soft perceptron algorithm for binary classification and in the one before that, we introduced the perceptron algorithm for binary and multi-class classification. In this assignment, we will introduce the soft perceptron algorithm for multi-class classification.\n","\n","Recall that, given an input feature vector $\\boldsymbol{f}(x) \\in \\mathbb{R}^d$, the multi-class perceptron algorithm **prediction** was defined as:\n","$$\n","\\begin{align*}\n","\\boldsymbol{s} &= \\begin{pmatrix}\n","w_1^{\\top} \\boldsymbol{f}(x) \\\\\n","w_2^{\\top} \\boldsymbol{f}(x) \\\\\n","\\vdots \\\\\n","w_K^{\\top} \\boldsymbol{f}(x)\n","\\end{pmatrix}\n","\\end{align*}\n","$$\n","where $w_k \\in \\mathbb{R}^d$ is the weight vector for class $k$. Then, to **decide** the class label, we would choose the class with the highest score:\n","$$\n","\\begin{align*}\n","% \\hat{y} &: \\mathbb{R}^d \\rightarrow \\{1, 2, \\ldots, K\\} \\\\\n","\\hat{y} &= \\arg\\max_{k} \\boldsymbol{s}_k\n","\\end{align*}\n","$$\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"nTzJywNV6A2D"},"source":["This time, for the soft perceptron algorithm, we will introduce the **soft-argmax** function, often referred to as the softmax function. The softargmax function is defined as:\n","$$\n","\\begin{align*}\n","\\text{softargmax} &: \\mathbb{R}^K \\rightarrow \\left( 0, 1 \\right)^K \\\\\n","\\text{softargmax}(\\boldsymbol{s}) &= \\frac{1}{\\sum_{k=1}^{K}\\exp(s_k)} \\begin{pmatrix}\n","\\exp(s_1) \\\\\n","\\exp(s_2) \\\\\n","\\vdots \\\\\n","\\exp(s_K)\n","\\end{pmatrix}\n","\\end{align*}\n","$$\n","\n","This function takes in input a $K$-dimensional vector of real numbers and outputs a $K$-dimensional vector of real numbers in the range $[0, 1]$ that **sum up to 1**. This enable us to interpret the output as a probability distribution over the $K$ classes. Moreover, the softargmax function can be parametrized by a coldness (or inverse temperature) parameter $\\beta$:\n","$$\n","\\begin{align*}\n","\\text{softargmax} &: \\mathbb{R}^K \\rightarrow \\left( 0, 1 \\right)^K \\\\\n","\\text{softargmax}_{}(\\boldsymbol{s}) &= \\frac{1}{\\sum_{k=1}^{K}\\exp(\\beta s_k)} \\begin{pmatrix}\n","\\exp(\\beta s_1) \\\\\n","\\exp(\\beta s_2) \\\\\n","\\vdots \\\\\n","\\exp(\\beta s_K)\n","\\end{pmatrix}\n","\\end{align*}\n","$$\n","\n","In this notebook we will always consider $\\beta = 1$.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"mjhLMQ_Z6A2E"},"source":["Having seen so, given a feature vector $\\boldsymbol{f}(x) \\in \\mathbb{R}^d$, the multi-class soft perceptron algorithm **prediction** is defined as:\n","$$\n","\\begin{align*}\n","\\tilde{\\boldsymbol{y}} &: \\mathbb{R}^d \\rightarrow \\left( 0, 1 \\right)^K \\\\\n","\\tilde{\\boldsymbol{y}} &= \\text{softargmax} \\left( \\begin{pmatrix}\n","\\boldsymbol{w}_1^{\\top} \\boldsymbol{f}(x) \\\\\n","\\boldsymbol{w}_2^{\\top} \\boldsymbol{f}(x) \\\\\n","\\vdots \\\\\n","\\boldsymbol{w}_K^{\\top} \\boldsymbol{f}(x)\n","\\end{pmatrix} \\right)\n","\\end{align*}\n","$$\n","where $\\boldsymbol{w}_k \\in \\mathbb{R}^d$ is the weight vector for class $k$.\n","\n","Then, to **decide** the class label, we would choose the class with the highest probability:\n","$$\n","\\begin{align*}\n","\\hat{y} &: \\mathbb{R}^d \\rightarrow \\{1, 2, \\ldots, K\\} \\\\\n","\\hat{y} &= \\arg\\max_{k} \\tilde{y}_k\n","\\end{align*}\n","$$\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Hjpso-PN6A2E"},"source":["The last ingredient we need to introduce is the loss function. In the last assignment, we introduced the **binary cross-entropy loss**. In this assignment, we will introduce the **cross-entropy loss**, which is a generalization of the binary cross-entropy loss to the multi-class case.\n","\n","First, we now that the total loss function is defined as the average of the per-sample loss function:\n","$$\n","\\mathcal{L}(\\boldsymbol{w}, \\mathcal{D}) = \\frac{1}{N} \\sum_{n=1}^{N} L(\\boldsymbol{w}, x_n, y_n)\n","$$\n","and this remains true.\n","\n","Second, we need to define the per-sample loss function. In the binary case, we defined the per-sample loss function as:\n","$$\n","L(\\boldsymbol{w}, x, y) = -y \\log(\\sigma[\\boldsymbol{w}^{\\top} \\boldsymbol{f}(x)]) - (1 - y) \\log(1 - \\sigma[\\boldsymbol{w}^{\\top} \\boldsymbol{f}(x)])\n","$$\n","where $\\sigma$ is the sigmoid function. Notice that this is the average over the two possible classes $y = 0$ and $y = 1$.\n","Now, the classes we have to consider are $K$ and the per-sample loss function is defined as:\n","$$\n","\\begin{align*}\n","L(\\boldsymbol{w}, x, y) &=  - \\log \\left( \\boldsymbol{y}^{\\top} \\tilde{\\boldsymbol{y}} \\right) \\\\\n","&= - \\log \\left( \\tilde{\\boldsymbol{y}}_{y} \\right) \\\\\n","&= - \\log \\left( \\text{softargmax} \\left( \\boldsymbol{w}^{\\top} \\boldsymbol{f}(x) \\right)_{y} \\right) \\\\\n","&= - \\log \\left( \\frac{\\exp( \\boldsymbol{w}_{y}^{\\top} \\boldsymbol{f}(x))}{\\sum_{k=1}^{K} \\exp(\\boldsymbol{w}_k^{\\top} \\boldsymbol{f}(x))} \\right) \\\\\n","&= -  \\boldsymbol{w}_y^{\\top} \\boldsymbol{f}(x) + \\log \\left( \\sum_{k=1}^{K} \\exp(\\boldsymbol{w}_k^{\\top} \\boldsymbol{f}(x)) \\right)\n","\\end{align*}\n","$$\n","where $\\boldsymbol{y} = \\text{one-hot}_K(y)$ is the one-hot encoded vector of the class label, $y$ is the true class label and we use the $\\boldsymbol{w}_y$ notation to denote the weight vector of the correct class label.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"G4LgiS4h6A2F"},"source":["The last thing we need is the update rule. We know that in general, the update rule is defined as:\n","$$\n","\\boldsymbol{W} \\leftarrow \\boldsymbol{W} - \\nabla_{\\boldsymbol{W}} \\mathcal{L}(\\boldsymbol{W}, \\mathcal{D})\n","$$\n","\n","Therefore we now explicitly compute the gradient of the loss function with respect to the weight matrix $W$:\n","$$\n","\\begin{align*}\n","\\nabla_{\\boldsymbol{W}} \\mathcal{L}(\\boldsymbol{W}, \\mathcal{D}) &= \\frac{1}{N} \\sum_{n=1}^{N} \\nabla_{\\boldsymbol{W}} L(\\boldsymbol{W}, x_n, y_n) \\\\\n","\\nabla_{\\boldsymbol{W}} L(\\boldsymbol{W}, x, y) &= \\begin{pmatrix}\n","\\nabla_{\\boldsymbol{w}_1} L(\\boldsymbol{w}_1, x, y) \\\\\n","\\nabla_{\\boldsymbol{w}_2} L(\\boldsymbol{w}_2, x, y) \\\\\n","\\vdots \\\\\n","\\nabla_{\\boldsymbol{w}_k} L(\\boldsymbol{w}_k, x, y)\n","\\end{pmatrix}  = \\begin{pmatrix}\n","0 \\\\\n","\\vdots \\\\\n","\\nabla_{\\boldsymbol{w}_y} \\boldsymbol{w}_y^{\\top} \\boldsymbol{f}(x) \\\\\n","\\vdots \\\\\n","0\n","\\end{pmatrix} + \\text{softargmax} \\left( \\boldsymbol{W}^{\\top} \\boldsymbol{f}(x) \\right) \\boldsymbol{f}^{\\top}(x) \\\\\n","\\nabla_{\\boldsymbol{W}} L(\\boldsymbol{W}, x, y) &= - \\boldsymbol{y}\\boldsymbol{f}^\\top(x) + \\text{softargmax} \\left( \\boldsymbol{W}^{\\top} \\boldsymbol{f}(x) \\right) \\boldsymbol{f}^\\top(x)\n","\\end{align*}\n","$$\n","\n","\n","We encourage you to try deriving this result yourself. Check out the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) for help with differentiating vectors and matrices.\n","\n","Therefore we have that the update rule is:\n","$$\n","\\begin{align*}\n","\\boldsymbol{W} &\\leftarrow \\boldsymbol{W} - \\nabla_{\\boldsymbol{W}} \\mathcal{L}(\\boldsymbol{W}, \\mathcal{D}) \\\\\n","\\boldsymbol{W} &\\leftarrow \\boldsymbol{W} + \\left( \\boldsymbol{y} - \\tilde{\\boldsymbol{y}} \\right) \\boldsymbol{f}(x)^{\\top}\n","\\end{align*}\n","$$\n","\n","where $\\tilde{\\boldsymbol{y}} = \\text{softargmax} \\left( \\boldsymbol{W}^{\\top} \\boldsymbol{f}(x) \\right)$."]},{"cell_type":"markdown","metadata":{"id":"JhwnYdMW6A2G"},"source":["### Exercise 1.1 - Multi-class Soft Perceptron\n","\n","The first exercise requires you to implement the multi-class soft perceptron algorithm using a Python class. Below is a template for the `MulticlassSoftPerceptron` class. You need to implement the methods `train`, `predict`, and `decide`:\n","\n","- The `train` method should implement the perceptron learning algorithm. It should take as input both the training data and the number of epochs to train for.\n","- The `predict` method should take a data point `f` as input and return the vector of probabilities $\\tilde{\\boldsymbol{y}}$.\n","- The `decide` method should take a data point `f` as input and return the predicted class label $\\hat{y}$.\n","\n","**Useful functions:**\n","- `numpy.outer`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SjYNH_e6A2G"},"outputs":[],"source":["class MulticlassSoftPerceptron:\n","\n","    @staticmethod\n","    def softargmax(s):\n","        raise NotImplementedError\n","\n","    def __init__(self, n_features, n_classes):\n","        # Save the input parameters\n","        self.n_classes = n_classes\n","        self.n_features = n_features\n","\n","        # Build the model parameters\n","        self.W = np.zeros((n_features, n_classes))\n","        self.b = np.zeros(n_classes)\n","\n","    def train(self, F, y, epochs):\n","        raise NotImplementedError\n","\n","    def predict(self, f):\n","        raise NotImplementedError\n","\n","    def decide(self, f):\n","        raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"pVHCKokO6A2H"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"HiDIVpiE6A2I"},"source":["### Exercise 1.2 - Cross Entropy\n","\n","The second exercise requires you to implement the cross-entropy loss function. Below is a template for the `cross_entropy_loss` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rwd7LYj96A2I"},"outputs":[],"source":["def cross_entropy_loss(y_true, y_pred):\n","    raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"uhqY5rP96A2J"},"source":["**Solution:**"]},{"cell_type":"markdown","metadata":{"id":"pJJ6YaLW6A2J"},"source":["### Exercise 1.3 - Putting it all together\n","\n","Now you can put in practice the `SoftPerceptron` class and the `cross_entropy_loss` function you implemented in the previous exercises.\n","\n","1. Load the Iris dataset\n","2. Split the dataset into training and test sets\n","3. Train the `SoftPerceptron` model on the training set\n","4. Evaluate the loss on both the training and test sets"]},{"cell_type":"markdown","metadata":{"id":"zsL1S2aE6A2K"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"AfDAYi2hN0vY"},"source":["## 2. AdaLinE\n","\n","AdaLinE (Adaptive Linear Neuron) is an adaptive filter, precursor of the single-layer neural networks. It attempts to model the relationship between two signals in real time in an iterative manner. It can be represented with the follwing block scheme\n","\n","<p align=\"center\">\n","  <img src=\"1 - Adaptive Filter.png\" alt=\"Image description\" width=\"60%\" />\n","</p>\n","<p align=\"center\"><i>Figure 1</i></p>\n","\n","A digital input signal $\\{x[n]\\}$ is fed into the filter, that computes a corresponding output signal $\\{\\tilde{y}[n]\\}$. The output signal is compared to a target signal $\\{y[n]\\}$ by subtraction, obtaining the error signal $\\{e[n]\\}$\n","\n","$$\n","\\begin{equation*}\n","    e[n] = y[n] - \\tilde{y}[n]\n","\\end{equation*}\n","$$\n","\n","The error signal is fed into a procedure which adapts the the response. This process of adaptation is represented by the oblique arrow that pierces the adaptive filter block in the *Figure 1*.\n","\n","In fact, the structure of the filter contains adjustable weights $\\boldsymbol{w}$ whose values affect how the output is computed. Regarding its structure, AdaLinE can be categorized as a finite response (FIR) filter. The structure of this type of device is described throught a linear relationship with a limited respone in time. So the filter is nothing more than a linear combination between the weight vector $\\boldsymbol{w}$ and the last $M$ samples of the input signal $\\{x[n]\\}$.\n","\n","$$\n","\\begin{equation*}\n","    \\tilde{y}[n] = \\boldsymbol{w}^\\top\\boldsymbol{f}[n] \\in \\mathbb{R}\n","\\end{equation*}\n","$$\n","\n","Where\n","\n","$$\n","\\begin{equation*}\n","    \\boldsymbol{w} = \\begin{bmatrix}w_0\\\\w_1\\\\ \\vdots \\\\w_{M-1}\\end{bmatrix} \\in \\mathbb{R}^M\n","\\end{equation*}\n","$$\n","<br>\n","$$\n","\\begin{equation*}\n","    \\boldsymbol{f}[n] = \\begin{bmatrix}x[n]\\\\x[n-1]\\\\ \\vdots \\\\x[n-M+1]\\end{bmatrix} \\in \\mathbb{R}^M\n","\\end{equation*}\n","$$\n","<br>\n","\n","A filter like AdaLinE can solve the general problem of system identification, depicted with the block scheme in the following figure.\n","\n","<p align=\"center\">\n","  <img src=\"2 - System Identification.png\" alt=\"Image description\" width=\"80%\" />\n","</p>\n","<p align=\"center\"><i>Figure 2</i></p>\n","\n","In this diagram, the input signal $\\{x[n]\\}$ is fed into an unknown system that produces the desired signal $\\{y[n]\\}$, with the additional effect of another signal $\\{s[n]\\}$. From the error signal $\\{e[n]\\}$, the filter can model the unknown system through its weights.\n","\n","In our context, the unknown system is represented by a room where the first speaker $\\{x[n]\\}$, emits a sound. The room introduces distortions to this signal, resulting in an room acoustics signal $\\{a[n]\\}$.\n","\n","The signal $\\{s[n]\\}$ represents a second speaker, which is the one we want to hear. The target signal $\\{y[n]\\}$ is a mixture of all these effects, being the sum of the room's influence and the second speaker's sound.\n","\n","The AdaLine output signal $\\{\\tilde{y}[n]\\}$ estimates the target. When subtracted from the target signal, we obtain the error signal $\\{e[n]\\}$ which, in this scenario, represents a sound that should be very close to the one we want to hear."]},{"cell_type":"markdown","metadata":{"id":"ZNpqBb9I90XD"},"source":["## The Method of Steepest Descent\n","The method of steepest descent is an optimization procedure for minimizing the value of a cost function with respect to a set of weights $\\boldsymbol{w}$. AdaLinE filter uses a particular form of this apporach, called least-mean-square (LMS) update algorithm.\n","\n","$$\n","\\begin{equation*}\n","    \\boldsymbol{w} \\gets \\boldsymbol{w} + \\eta \\space (y[n] - \\tilde{y}[n])\\boldsymbol{f}[n]\n","\\end{equation*}\n","$$\n","\n","Here $\\eta$ is the learning rate, so called because it determines the magnitude of the change that is taken by the algorithm in iteratively determining useful weights. This is a very delicate value that can easily influence the success of the method."]},{"cell_type":"markdown","metadata":{"id":"VoYSkXAJ7lXk"},"source":["### Exercise 2.1 - Room system\n","In this first exercise, you are asked to set up the two systems that will later be used in the notebook. The first to be defined is the Room class, which implements the unknown system in Figure 2. This class must be such that:\n","\n","- The first part of the room system parameters contains information about the delay introduced. To introduce a delay of `delay_samples` samples, it is necessary to insert a unit value after a series of zeros equal to the number of delay samples, for each signal repetition.\n","\n","- The second part of the room system parameters contains information about the introduced reverberation. Since reverberation is like the signal repeating itself in a weaker and weaker manner, to implement it, it is necessary to place a decreasing intensity below the unit value after the noise samples.\n","\n","- The last part of the parameters must contain padding zeros.\n","\n","This system, like the AdaLinE filter that follows it, works with a type of signal called a tapped delay line $F$. A tapped delay line essentially samples the input signal by selecting sliding windows of its values, with each window shifted by one position for each successive sample. These elements are nothing more than the $\\boldsymbol{f}[n]$ samples.\n","\n","<br>\n","$$\n","\\begin{equation*}\n","    F = \\begin{bmatrix}\\boldsymbol{f}^\\top[0] \\\\ \\boldsymbol{f}^\\top[1] \\\\ \\vdots \\\\ \\boldsymbol{f}^\\top[N-1] \\end{bmatrix} \\in \\mathbb{R}^{N \\times M}\n","\\end{equation*}\n","$$\n","<br>\n","\n","With this in mind, It is possible to implement the `room_effect` method,In which the effect, and therefore the output, of the system is applied just like an adaptive filter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XM2_0mEolZRo"},"outputs":[],"source":["class Room:\n","    def __init__(self,delay_samples,reverb_samples):\n","        # Room parameters\n","        self.pad_right = delay_samples - 1\n","        total_params = delay_samples + reverb_samples + self.pad_right\n","        if total_params % 2 == 0:\n","            total_params += 1\n","\n","        self.valid_params = delay_samples + reverb_samples\n","        self.room_params = np.zeros((total_params,1))\n","\n","        # Delay\n","        ... # TODO\n","\n","        # Reverb\n","        ... # TODO\n","\n","        # Right padding\n","        ... # TODO\n","\n","    def room_effect(self, x):\n","        N = len(x)\n","        room_acoustic = np.zeros(N)\n","\n","        for n in tqdm(range(N), desc=\"Applying room effect\"):\n","            # Extract window: f[n] = [x[n], x[n-1], ..., x[n-M+1]]^T\n","            fn = np.zeros((len(self.room_params),1))\n","\n","            for i in range(len(self.room_params)):\n","                idx = n - i\n","                if idx >= 0:\n","                    fn[i] = x[idx]\n","\n","            # Training step\n","            room_acoustic[n] = (...).item() # TODO\n","\n","        return room_acoustic.reshape((-1,1))\n","\n","    def room_parameters(self):\n","        return self.room_params"]},{"cell_type":"markdown","metadata":{"id":"3wyi_viWlX2c"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"YsMoeHxu2r12"},"source":["### Exercise 2.2 - AdaLinE filter\n","Once we have defined the signals, we need to define the tool with which to manipulate them, i.e. the AdaLinE filter.\n","\n","Based on this, in this exercise, you are tasked with implementing the `ADALINE` class that realizes this entity. In particular, based on the concepts of FIR filters, the class must include:\n","\n","- the `predict` method that returns the estimated target signal $\\{\\tilde{y}[n]\\}$, given the input signal.\n","- the `train_step` method implements an iteration of the LMS method (remember to save the error $e[n]  = y[n] - \\tilde{y}[n]$ in this case) and returns the estimation of the filter at that step.\n","- the `train` for each sample of the input signal, apply the LMS update (hint: it works exactly like the room effect...)\n","- the `apply` method, which, given the input signal, returns the signal\n","estimated by the filter (hint: again, remember how the room effect works)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2x-JYToCTV9"},"outputs":[],"source":["class ADALINE:\n","    def __init__(self, M, eta):\n","        self.M = M\n","        self.eta = eta\n","        self.w = np.zeros((M,1))\n","        self.errors = []\n","\n","    def predict(self, fn):\n","        return NotImplementedError\n","\n","    def train_step(self, fn, yn):\n","        return NotImplementedError\n","\n","    def train(self, x, y):\n","        raise NotImplementedError\n","\n","    def apply(self,x):\n","        return NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"U7JJq8mgf7fP"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"3QumoxODdyCf"},"source":["### Exercise 2.3 - System setup\n","This second exercise addresses an application scenario. In this case, it is essential to set up the signals required for the experiment, as illustrated in *Figure 2*:\n","\n","- The input signal $\\{x[n]\\}$ represents the first speaker and is modeled as a square wave signal with a fundamental frequency of $1000~\\textrm{Hz}$.\n","\n","- The sound signal $\\{s[n]\\}$, represents the second speaker and is modeled as a sinusoidal signal with a frequency of $554~\\textrm{Hz}$. The amplitude of this signal is slightly lower than that of $\\{x[n]\\}$\n","\n","- The audio signal $\\{a[n]\\}$ is the output of the room system, which includes both the delay of \\($4$ samples\\) and reverb effects ($6$ samples\\) applied to the first speaker's signal $\\{x[n]\\}$. This signal must be of the same length of the sound signal\n","\n","- The target signal $\\{y[n]\\}$ that sums the audio signal and the second speaker signal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNALFXE3eX3i"},"outputs":[],"source":["# Signals parameters\n","fs = 8000  # Hz\n","duration = 0.5  # seconds\n","t = np.linspace(0, duration, int(fs * duration)) # Time interval"]},{"cell_type":"markdown","metadata":{"id":"ebjHNy2KefLP"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"96FhamgxfsIj"},"source":["### Exercise 2.4 - Learning delay and reverb\n","We now need to update the filter weights so that the AdaLinE filter can approximate the response of the unknown room system:\n","\n","- Determine how many filter weights $\\boldsymbol{w}$ are needed to effectively model the system\n","- Choose a suitable learning rate $\\eta$ for the weight update algorithm, ensuring the filter converges without overshooting\n","- Initialize the filter and train it\n","- Plot the error curve using the stored error values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J56x0Qoe2Gyw"},"outputs":[],"source":["def plot_error_curve(errors):\n","    # Figure\n","    fig, ax = plt.subplots(figsize=(12, 5))\n","\n","    # Error plot data\n","    errors_squared = np.array(errors) ** 2\n","    n = np.arange(len(errors))\n","\n","    # Plot moving average for a smoother view\n","    window = min(100, len(errors) // 10)\n","    mse_smooth = np.convolve(errors_squared, np.ones(window)/window, mode=\"valid\")\n","    ax.plot(n[window-1:], mse_smooth, \"darkblue\", linewidth=1)\n","\n","    ax.set_xlabel(\"Iteration\", fontsize=11)\n","    ax.set_ylabel(\"Squared Error\", fontsize=11)\n","    ax.set_title(\"Average error during train\", fontsize=12, fontweight=\"bold\")\n","    ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S0LKZketkPLl"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"izD9fIiifsfI"},"source":["### Exercise 2.5 - Evaluation\n","Visualize the weights and signals to check the filter behavior. If everything is working correctly, you should be able to see AdeLinE weights that resemble the parameters of the room system and the error signal $\\{e[n]\\}$ closely resembles the second speaker signal $\\{s[n]\\}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ot2xnioEntcJ"},"outputs":[],"source":["def plot_weights(room_parameters, w):\n","    fig, axes = plt.subplots(2, 1, figsize=(10, 6))\n","\n","    titles = [\"Room (room_parameters)\", \"ADALINE (w)\"]\n","    colors = [\"orange\", \"red\"]\n","    ws = [room_parameters, w]\n","\n","    for ax, w, title, color in zip(axes, ws, titles, colors):\n","        n = np.arange(len(w))\n","        markers, stems, base = ax.stem(n, w, basefmt=\" \")\n","        plt.setp(stems, linewidth=1.8, color=color)\n","        plt.setp(markers, markersize=6, color=color, marker=\"o\")\n","\n","        ax.set_title(title, fontsize=12, fontweight=\"bold\")\n","        ax.set_xlabel(\"Index\")\n","        ax.set_ylabel(\"Amplitude\")\n","        ax.grid(True, alpha=0.3)\n","        ax.axhline(y=0, color=\"k\", linewidth=0.8)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Acvz920C2C2E"},"outputs":[],"source":["def plot_signals(x, y, y_pred, s, a, e, zoom=True):\n","    # Figure regulation (to show significative sections)\n","    N = len(x)\n","    n = np.arange(N)\n","\n","    if zoom:\n","        zoom_samples=(1000, 1500)\n","        n_start, n_end = zoom_samples\n","        n_start = max(0, n_start)\n","        n_end = min(N, n_end)\n","    else:\n","        n_start, n_end = 0, N\n","\n","    n = np.arange(N)[n_start:n_end]\n","\n","    # Figure\n","    fig, ax = plt.subplots(5, 1, figsize=(14, 12))\n","\n","    # Input signal x[n]\n","    ax[0].plot(n, x[n_start:n_end], \"blue\", linewidth=1, alpha=0.7)\n","    ax[0].set_ylabel(\"x[n]\", fontsize=11)\n","    ax[0].set_title(f\"Input signal - Speaker 1\", fontsize=12, fontweight=\"bold\")\n","    ax[0].set_xlabel(\"n\", fontsize=11)\n","    ax[0].grid(True, alpha=0.3)\n","\n","    # Sound signal s[n]\n","    ax[1].plot(n, s[n_start:n_end], \"orange\", linewidth=1, alpha=0.7)\n","    ax[1].set_ylabel(\"s[n]\", fontsize=11)\n","    ax[1].set_title(\"Sound Signal - Speaker 2\", fontsize=12, fontweight=\"bold\")\n","    ax[1].set_xlabel(\"n\", fontsize=11)\n","    ax[1].grid(True, alpha=0.3)\n","\n","    # Room output a[n]\n","    ax[2].plot(n, a[n_start:n_end], \"purple\", linewidth=1, alpha=0.7)\n","    ax[2].set_ylabel(\"a[n]\", fontsize=11)\n","    ax[2].set_title(\"Audio Signal - Room Output\", fontsize=12, fontweight=\"bold\")\n","    ax[2].set_xlabel(\"n\", fontsize=11)\n","    ax[2].grid(True, alpha=0.3)\n","\n","    # Target signal y[n] vs Filter output \\tilde{y}[n]\n","    ax[3].plot(n, y[n_start:n_end], \"green\", linewidth=1, alpha=0.7, label=\"y[n] = a[n] + s[n]\")\n","    ax[3].plot(n, y_pred[n_start:n_end], \"r--\", linewidth=1, alpha=0.8, label=r\"$\\tilde{y}[n]$ (filter output)\")\n","    ax[3].set_ylabel(r\"y[n]/$\\tilde{y}[n]$\", fontsize=11)\n","    ax[3].set_xlabel(\"n\", fontsize=11)\n","    ax[3].set_title(\"Target Signal vs Filter output\", fontsize=12, fontweight=\"bold\")\n","    ax[3].legend(loc=\"upper right\")\n","    ax[3].grid(True, alpha=0.3)\n","\n","    # Error signal e[n] - What we want to listen\n","    ax[4].plot(n, e[n_start:n_end], \"darkgreen\", linewidth=0.8, alpha=0.7)\n","    ax[4].set_ylabel(\"e[n]\", fontsize=11)\n","    ax[4].set_xlabel(\"n\", fontsize=11)\n","    ax[4].set_title(\"Error Signal (ideally ≈ {s[n]})\", fontsize=12, fontweight=\"bold\")\n","    ax[4].grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MK71d7y2Nnlt"},"source":["Another clear way to verify if everything has worked is by listening to the signals. Use the save_audio function along with `IPython package` (`Audio` and `display`) to listen to the signal and check if $\\{e[n]\\}$ resembles the sound from the second speaker $\\{s[n]\\}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-mvAnRYvV3O"},"outputs":[],"source":["def save_audio(filename, signal, fs):\n","    signal_normalized = np.int16(signal / np.max(np.abs(signal)) * 32767 * 0.9)\n","    wavfile.write(filename, fs, signal_normalized)"]},{"cell_type":"markdown","metadata":{"id":"fpLLbZeKmAdI"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"MaMalEk552MS"},"source":["## 3. Listening is believing"]},{"cell_type":"markdown","metadata":{"id":"hH4bBkQdmz4z"},"source":["This final exercise focuses on using AdaLinE with WAV signals, providing a practical way to understand how these tools can be applied in real-world scenarios.\n","\n","\n","### Exercise 3.1 - A personalized example\n","\n","First of all you need to load the signal $\\{x[n]\\}$ (*one.wav*) and $\\{s[n]\\}$ (*two.wav*). Note that the WAV files must be present in the same directory as this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmhFpaxB36uT"},"outputs":[],"source":["def load_sound(wav_file):\n","    fs, x = wavfile.read(wav_file)\n","\n","    # Conversion and normalization\n","    if x.dtype == np.int16:\n","        x = x.astype(np.float32) / 32768.0\n","    elif x.dtype == np.int32:\n","        x = x.astype(np.float32) / 2147483648.0\n","    else:\n","        x = x.astype(np.float32)\n","\n","    # If stereo, just one channel\n","    if len(x.shape) > 1:\n","        x = x[:, 0]\n","\n","    return x,fs"]},{"cell_type":"markdown","metadata":{"id":"3XRqqnKWrxPs"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"ZzDqaVHgrCJS"},"source":["The first $8000$ samples of the two signals are zero and must be eliminated. Furthermore, the two signals must have the same number of samples. The room's acoustic signal $\\{a[n]\\}$ must be set with a delay of $20$ samples and a reverberation of $6$ samples. The signal $\\{y[n]\\}$ is obtained as in the previous case."]},{"cell_type":"markdown","metadata":{"id":"ctjqRJoMsKT6"},"source":["**Solution**"]},{"cell_type":"markdown","metadata":{"id":"yxsAZuT6sSNt"},"source":["### 3.2 AdaLinE pipeline\n","\n","At this point, you can apply the pipeline from the previous exercise: train AdaLinE, plot weights and signals, and listen to the resulting signals. To better visualize the signals, in this case remember to set the plot function parameter `zoom=False`."]},{"cell_type":"markdown","metadata":{"id":"GXnnGgvdm3kq"},"source":["**Solution**"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}